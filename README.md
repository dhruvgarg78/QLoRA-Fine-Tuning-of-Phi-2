# QLoRA Fine-Tuning of Phi-2

This repository contains an experiment fine-tuning **[microsoft/phi-2](https://huggingface.co/microsoft/phi-2)** using **QLoRA** (Quantized Low-Rank Adaptation). The fine-tuning is performed on the **OpenAssistant dataset**, targeting instruction-following alignment.

---

## ğŸ“Œ Project Overview

* **Base Model**: Phi-2 (2.7B parameters)
* **Fine-Tuning Method**: QLoRA with 4-bit quantization (bnb.int4)
* **Frameworks**: Hugging Face `transformers`, `trl`, `peft`
* **Dataset**: OpenAssistant conversations (instruction/response pairs)
* **Compute**: NVIDIA T4 (suitable for lightweight experimentation)

---

## âš™ï¸ Training Configuration

* **Sequence Length**: 256 tokens
* **Training Examples**: Capped at 32,000 for ~3â€“5 hour runs
* **Preview Prompt**: Generated every 100 optimizer steps to monitor progress
* **Quantization**: 4-bit loading of base model + LoRA adapters
* **Optimizer**: AdamW (via Hugging Face `trl` defaults)
* **Output Directory**: `phi2-oasst1-qlora`

---


## ğŸš€ Training Pipeline

1. **Load Dataset** â€“ Preprocess JSONL input with text-only entries.
2. **Tokenizer Setup** â€“ Phi-2 tokenizer, extended with special tokens.
3. **Model Load** â€“ Quantized Phi-2 (bnb int4).
4. **Apply LoRA** â€“ Inject LoRA adapters on attention/MLP layers.
5. **Train with TRLâ€™s `SFTTrainer`** â€“ Monitor loss and generate periodic previews.
6. **Save Checkpoints** â€“ Adapters stored incrementally for resumption.

---

## ğŸ” Example Behavior

**Before Fine-Tuning (vanilla Phi-2):**

* Responses are generic, verbose, and not instruction-following.

**After Fine-Tuning (QLoRA on OASST):**

* Produces structured, polite, instruction-following completions.
* Tends toward verbose answers due to dataset style.
* Example: Email formatting task now yields multi-step structured responses.

---

## ğŸ“Š Key Observations

* **Logs confirm correct weight loading**: All checkpoints matched.
* **Fine-tuning effect**: Noticeable shift in style to instruction-following, but sometimes ignores strict word limits (dataset bias).
* **Preview generations**: Help monitor improvement in structure and tone.

---

## ğŸ› ï¸ Future Work

* Add dataset samples emphasizing **conciseness** (â‰¤100 words) to enforce brevity.
* Experiment with **DPO/GRPO** for reward-driven fine-tuning on â€œconcisenessâ€.
* Evaluate outputs on custom held-out prompts.

---

## ğŸ“ References

* [Phi-2 on Hugging Face](https://huggingface.co/microsoft/phi-2)
* [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
* [TRL (Transformers Reinforcement Learning)](https://github.com/huggingface/trl)
* [OpenAssistant Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)